"""
Model identification capabilities for watermark detection.

This module provides functionality to identify the source model used to generate
watermarked text based on statistical patterns, model signatures, and detection
confidence scores.
"""

import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class ModelFamily(Enum):
    """Enumeration of supported model families."""
    GEMMA = "gemma"
    GPT = "gpt"
    LLAMA = "llama"
    CLAUDE = "claude"
    UNKNOWN = "unknown"


@dataclass
class ModelSignature:
    """
    Statistical signature characteristics of a specific model.
    
    This class encapsulates the statistical patterns and characteristics
    that can be used to identify text generated by a specific model.
    
    Attributes:
        model_name (str): Full model identifier
        family (ModelFamily): Model family classification
        typical_perplexity_range (Tuple[float, float]): Expected perplexity range
        token_distribution_entropy (float): Typical token distribution entropy
        repetition_patterns (Dict[str, float]): Characteristic repetition patterns
        sequence_coherence (float): Typical sequence coherence score
        confidence_threshold (float): Minimum confidence for identification
    """
    model_name: str
    family: ModelFamily
    typical_perplexity_range: Tuple[float, float]
    token_distribution_entropy: float
    repetition_patterns: Dict[str, float]
    sequence_coherence: float
    confidence_threshold: float = 0.7


class ModelIdentifier:
    """
    Identifies source models based on text analysis and detection patterns.
    
    This class analyzes statistical patterns in text and detection results
    to identify the most likely source model used for generation. It uses
    model signatures and pattern matching for identification.
    """
    
    def __init__(self):
        """Initialize the model identifier with known model signatures."""
        self.model_signatures = self._initialize_model_signatures()
        self.identification_history: List[Dict[str, Any]] = []
        
        logger.info(f"Initialized ModelIdentifier with {len(self.model_signatures)} model signatures")
    
    def _initialize_model_signatures(self) -> Dict[str, ModelSignature]:
        """
        Initialize database of known model signatures.
        
        Returns:
            Dict[str, ModelSignature]: Dictionary mapping model names to signatures
        """
        signatures = {}
        
        # Google Gemma models
        signatures["google/gemma-2-2b"] = ModelSignature(
            model_name="google/gemma-2-2b",
            family=ModelFamily.GEMMA,
            typical_perplexity_range=(8.0, 25.0),
            token_distribution_entropy=6.5,
            repetition_patterns={
                "immediate_repetition": 0.02,
                "bigram_repetition": 0.15,
                "trigram_repetition": 0.08
            },
            sequence_coherence=7.2,
            confidence_threshold=0.75
        )
        
        signatures["google/gemma-2-9b"] = ModelSignature(
            model_name="google/gemma-2-9b",
            family=ModelFamily.GEMMA,
            typical_perplexity_range=(6.0, 20.0),
            token_distribution_entropy=7.0,
            repetition_patterns={
                "immediate_repetition": 0.015,
                "bigram_repetition": 0.12,
                "trigram_repetition": 0.06
            },
            sequence_coherence=7.8,
            confidence_threshold=0.8
        )
        
        signatures["google/gemma-2-27b"] = ModelSignature(
            model_name="google/gemma-2-27b",
            family=ModelFamily.GEMMA,
            typical_perplexity_range=(4.0, 15.0),
            token_distribution_entropy=7.5,
            repetition_patterns={
                "immediate_repetition": 0.01,
                "bigram_repetition": 0.10,
                "trigram_repetition": 0.05
            },
            sequence_coherence=8.2,
            confidence_threshold=0.85
        )
        
        # Microsoft Phi models (for comparison/fallback)
        signatures["microsoft/Phi-3-mini-4k-instruct"] = ModelSignature(
            model_name="microsoft/Phi-3-mini-4k-instruct",
            family=ModelFamily.GPT,
            typical_perplexity_range=(10.0, 30.0),
            token_distribution_entropy=6.0,
            repetition_patterns={
                "immediate_repetition": 0.025,
                "bigram_repetition": 0.18,
                "trigram_repetition": 0.10
            },
            sequence_coherence=6.8,
            confidence_threshold=0.7
        )
        
        return signatures
    
    def identify_model(
        self,
        detection_metadata: Dict[str, Any],
        confidence_score: float,
        text_length: int
    ) -> Tuple[Optional[str], float]:
        """
        Identify the most likely source model based on detection results.
        
        This method analyzes detection metadata and statistical patterns
        to determine which model most likely generated the input text.
        
        Args:
            detection_metadata (Dict[str, Any]): Metadata from detection analysis
            confidence_score (float): Overall detection confidence score
            text_length (int): Length of analyzed text
            
        Returns:
            Tuple[Optional[str], float]: Identified model name and identification confidence
        """
        try:
            # Extract relevant metrics from detection metadata
            analysis_metrics = self._extract_analysis_metrics(detection_metadata)
            
            if not analysis_metrics:
                logger.warning("Insufficient metrics for model identification")
                return None, 0.0
            
            # Calculate similarity scores for each known model
            model_scores = {}
            
            for model_name, signature in self.model_signatures.items():
                similarity_score = self._calculate_model_similarity(
                    analysis_metrics, signature, confidence_score, text_length
                )
                model_scores[model_name] = similarity_score
            
            # Find best matching model
            if not model_scores:
                return None, 0.0
            
            best_model = max(model_scores.items(), key=lambda x: x[1])
            best_model_name, best_score = best_model
            
            # Apply confidence threshold
            signature = self.model_signatures[best_model_name]
            if best_score < signature.confidence_threshold:
                logger.info(f"Best match {best_model_name} below threshold: {best_score:.3f}")
                return None, best_score
            
            # Record identification for history
            self._record_identification(
                best_model_name, best_score, analysis_metrics, model_scores
            )
            
            logger.info(f"Identified model: {best_model_name} (confidence: {best_score:.3f})")
            
            return best_model_name, best_score
            
        except Exception as e:
            logger.error(f"Model identification failed: {e}")
            return None, 0.0
    
    def _extract_analysis_metrics(self, metadata: Dict[str, Any]) -> Optional[Dict[str, float]]:
        """
        Extract relevant metrics from detection metadata.
        
        Args:
            metadata (Dict[str, Any]): Detection metadata dictionary
            
        Returns:
            Optional[Dict[str, float]]: Extracted metrics or None if insufficient data
        """
        try:
            # Extract detection scores
            detection_scores = metadata.get("detection_scores", {})
            
            # Extract text analysis metrics
            text_analysis = metadata.get("text_analysis", {})
            
            # Compile metrics for model identification
            metrics = {
                "perplexity": detection_scores.get("perplexity", 0.5),
                "token_distribution_entropy": detection_scores.get("token_distribution", 0.5),
                "repetition_score": detection_scores.get("repetition_patterns", 0.5),
                "sequence_coherence": detection_scores.get("sequence_coherence", 0.5),
                "token_count": text_analysis.get("token_count", 0),
                "unique_token_ratio": self._calculate_unique_token_ratio(text_analysis),
                "avg_token_length": text_analysis.get("avg_token_length", 0.0)
            }
            
            # Validate metrics
            if metrics["token_count"] == 0:
                return None
            
            return metrics
            
        except Exception as e:
            logger.warning(f"Failed to extract analysis metrics: {e}")
            return None
    
    def _calculate_unique_token_ratio(self, text_analysis: Dict[str, Any]) -> float:
        """
        Calculate ratio of unique tokens to total tokens.
        
        Args:
            text_analysis (Dict[str, Any]): Text analysis metadata
            
        Returns:
            float: Unique token ratio (0.0-1.0)
        """
        token_count = text_analysis.get("token_count", 0)
        unique_tokens = text_analysis.get("unique_tokens", 0)
        
        if token_count == 0:
            return 0.0
        
        return unique_tokens / token_count
    
    def _calculate_model_similarity(
        self,
        metrics: Dict[str, float],
        signature: ModelSignature,
        confidence_score: float,
        text_length: int
    ) -> float:
        """
        Calculate similarity between analysis metrics and model signature.
        
        Args:
            metrics (Dict[str, float]): Extracted analysis metrics
            signature (ModelSignature): Model signature to compare against
            confidence_score (float): Overall detection confidence
            text_length (int): Length of analyzed text
            
        Returns:
            float: Similarity score (0.0-1.0)
        """
        try:
            similarity_scores = []
            
            # Perplexity similarity
            perplexity_sim = self._calculate_perplexity_similarity(
                metrics["perplexity"], signature.typical_perplexity_range
            )
            similarity_scores.append(("perplexity", perplexity_sim, 0.3))
            
            # Token distribution entropy similarity
            entropy_sim = self._calculate_entropy_similarity(
                metrics["token_distribution_entropy"], signature.token_distribution_entropy
            )
            similarity_scores.append(("entropy", entropy_sim, 0.25))
            
            # Repetition pattern similarity
            repetition_sim = self._calculate_repetition_similarity(
                metrics["repetition_score"], signature.repetition_patterns
            )
            similarity_scores.append(("repetition", repetition_sim, 0.2))
            
            # Sequence coherence similarity
            coherence_sim = self._calculate_coherence_similarity(
                metrics["sequence_coherence"], signature.sequence_coherence
            )
            similarity_scores.append(("coherence", coherence_sim, 0.15))
            
            # Confidence score bonus (higher confidence = more reliable identification)
            confidence_bonus = confidence_score * 0.1
            similarity_scores.append(("confidence", confidence_bonus, 0.1))
            
            # Calculate weighted average
            total_weighted_score = sum(score * weight for _, score, weight in similarity_scores)
            total_weight = sum(weight for _, _, weight in similarity_scores)
            
            if total_weight == 0:
                return 0.0
            
            final_similarity = total_weighted_score / total_weight
            
            # Apply text length penalty for very short texts
            if text_length < 100:
                length_penalty = text_length / 100.0
                final_similarity *= length_penalty
            
            return min(1.0, max(0.0, final_similarity))
            
        except Exception as e:
            logger.warning(f"Similarity calculation failed: {e}")
            return 0.0
    
    def _calculate_perplexity_similarity(
        self,
        observed_perplexity: float,
        expected_range: Tuple[float, float]
    ) -> float:
        """
        Calculate similarity based on perplexity values.
        
        Args:
            observed_perplexity (float): Observed perplexity score
            expected_range (Tuple[float, float]): Expected perplexity range
            
        Returns:
            float: Perplexity similarity score (0.0-1.0)
        """
        min_perp, max_perp = expected_range
        
        # Convert normalized perplexity back to actual range for comparison
        # (This is a simplified conversion - in practice, you'd use actual perplexity values)
        estimated_perplexity = (1.0 - observed_perplexity) * 100
        
        if min_perp <= estimated_perplexity <= max_perp:
            return 1.0
        
        # Calculate distance from range
        if estimated_perplexity < min_perp:
            distance = min_perp - estimated_perplexity
        else:
            distance = estimated_perplexity - max_perp
        
        # Convert distance to similarity (exponential decay)
        similarity = max(0.0, 1.0 - (distance / 50.0))
        return similarity
    
    def _calculate_entropy_similarity(
        self,
        observed_entropy: float,
        expected_entropy: float
    ) -> float:
        """
        Calculate similarity based on token distribution entropy.
        
        Args:
            observed_entropy (float): Observed entropy score
            expected_entropy (float): Expected entropy value
            
        Returns:
            float: Entropy similarity score (0.0-1.0)
        """
        # Convert normalized entropy back to actual range
        estimated_entropy = observed_entropy * 10.0
        
        # Calculate absolute difference
        difference = abs(estimated_entropy - expected_entropy)
        
        # Convert to similarity (exponential decay)
        similarity = max(0.0, 1.0 - (difference / 5.0))
        return similarity
    
    def _calculate_repetition_similarity(
        self,
        observed_repetition: float,
        expected_patterns: Dict[str, float]
    ) -> float:
        """
        Calculate similarity based on repetition patterns.
        
        Args:
            observed_repetition (float): Observed repetition score
            expected_patterns (Dict[str, float]): Expected repetition patterns
            
        Returns:
            float: Repetition similarity score (0.0-1.0)
        """
        # Use average of expected patterns as baseline
        expected_avg = sum(expected_patterns.values()) / len(expected_patterns)
        
        # Calculate similarity
        difference = abs(observed_repetition - expected_avg)
        similarity = max(0.0, 1.0 - (difference * 2.0))
        
        return similarity
    
    def _calculate_coherence_similarity(
        self,
        observed_coherence: float,
        expected_coherence: float
    ) -> float:
        """
        Calculate similarity based on sequence coherence.
        
        Args:
            observed_coherence (float): Observed coherence score
            expected_coherence (float): Expected coherence value
            
        Returns:
            float: Coherence similarity score (0.0-1.0)
        """
        # Convert normalized coherence back to actual range
        estimated_coherence = observed_coherence * 8.0
        
        # Calculate similarity
        difference = abs(estimated_coherence - expected_coherence)
        similarity = max(0.0, 1.0 - (difference / 4.0))
        
        return similarity
    
    def _record_identification(
        self,
        identified_model: str,
        confidence: float,
        metrics: Dict[str, float],
        all_scores: Dict[str, float]
    ) -> None:
        """
        Record identification result for analysis and improvement.
        
        Args:
            identified_model (str): Name of identified model
            confidence (float): Identification confidence
            metrics (Dict[str, float]): Analysis metrics used
            all_scores (Dict[str, float]): Scores for all models
        """
        record = {
            "identified_model": identified_model,
            "confidence": confidence,
            "metrics": metrics.copy(),
            "all_model_scores": all_scores.copy(),
            "timestamp": time.time()
        }
        
        self.identification_history.append(record)
        
        # Keep only recent history (last 1000 identifications)
        if len(self.identification_history) > 1000:
            self.identification_history = self.identification_history[-1000:]
    
    def get_supported_models(self) -> List[str]:
        """
        Get list of models that can be identified.
        
        Returns:
            List[str]: List of supported model identifiers
        """
        return list(self.model_signatures.keys())
    
    def get_model_families(self) -> List[str]:
        """
        Get list of supported model families.
        
        Returns:
            List[str]: List of model family names
        """
        families = set(sig.family.value for sig in self.model_signatures.values())
        return list(families)
    
    def add_model_signature(self, signature: ModelSignature) -> None:
        """
        Add a new model signature to the identification database.
        
        Args:
            signature (ModelSignature): New model signature to add
        """
        self.model_signatures[signature.model_name] = signature
        logger.info(f"Added model signature for {signature.model_name}")
    
    def get_identification_stats(self) -> Dict[str, Any]:
        """
        Get statistics about model identification performance.
        
        Returns:
            Dict[str, Any]: Identification statistics and metrics
        """
        if not self.identification_history:
            return {"total_identifications": 0}
        
        # Calculate statistics
        total_identifications = len(self.identification_history)
        
        # Model frequency
        model_counts = {}
        confidence_scores = []
        
        for record in self.identification_history:
            model = record["identified_model"]
            model_counts[model] = model_counts.get(model, 0) + 1
            confidence_scores.append(record["confidence"])
        
        # Calculate average confidence
        avg_confidence = sum(confidence_scores) / len(confidence_scores)
        
        return {
            "total_identifications": total_identifications,
            "model_distribution": model_counts,
            "average_confidence": avg_confidence,
            "supported_models": len(self.model_signatures),
            "supported_families": len(self.get_model_families())
        }


# Import time for timestamp recording
import time